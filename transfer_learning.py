# -*- coding: utf-8 -*-
"""BitirmeProjesi-TransferLearning.ipynb

Automatically generated by Colaboratory.

#Installs
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install mtcnn
!pip install keras_vggface keras_applications
!pip install split_folders

import torch

# If there's a GPU available...
if torch.cuda.is_available():    

    # Tell PyTorch to use the GPU.    
    device = torch.device("cuda")

    print('There are %d GPU(s) available.' % torch.cuda.device_count())
    print('We will use the GPU:', torch.cuda.get_device_name(0))

# If not...
else:
    print('No GPU available, using the CPU instead.')
    device = torch.device("cpu")

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import datasets, layers, models, initializers
from mtcnn.mtcnn import MTCNN
import cv2
import numpy as np
from keras.preprocessing import image
from keras_vggface.utils import preprocess_input
from keras_vggface.vggface import VGGFace
from scipy.spatial.distance import cosine
from keras_preprocessing.image import ImageDataGenerator
from google.colab.patches import cv2_imshow
from keras.models import load_model
from keras.optimizers import SGD
from tensorflow.keras.utils import plot_model
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Flatten, GlobalAveragePooling2D
from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D
from keras.layers.normalization import BatchNormalization
from keras.models import Model
from keras_vggface import utils
from sklearn.model_selection import train_test_split
import splitfolders
import os

import warnings
warnings.filterwarnings("ignore", category=FutureWarning)

! unzip drive/MyDrive/face-dataset-new.zip

"""#Â Split the dataset"""

data_dir = '/content/CASIA-WebFace'
splitfolders.ratio(data_dir, output='output', seed=1337, ratio=(.8, .2), group_prefix=None)

"""#Generator"""

# Take the train and validation data
train_datagen = ImageDataGenerator(
    rescale=1./255,
    validation_split=0.25,
)

# Take the train and validation data
test_datagen = ImageDataGenerator(
    rescale=1./255,
)

batch_size = 128

train_generator = train_datagen.flow_from_directory(
    directory='/content/output/train',
    target_size=(224,224),
    class_mode='categorical',
    batch_size=batch_size, 
    subset='training'
    )

validation_generator = train_datagen.flow_from_directory(
    directory='/content/output/train',
    target_size=(224,224),
    batch_size=batch_size,
    class_mode='categorical',
    subset='validation'
    )

test_generator = test_datagen.flow_from_directory(
    directory='/content/output/val',
    #shuffle = False,
    target_size=(224,224),
    batch_size=batch_size,
    class_mode='categorical',
    )

"""#Transfer Learning

"""

vgg_model = VGGFace(include_top=False, input_shape=(224, 224, 3))

def addModel(bottom_model, num_classes):
  for layer in bottom_model.layers:
    layer.trainable = False

  hidden_dim = 512

  last_layer = vgg_model.get_layer('pool5').output
  x = Flatten(name='flatten')(last_layer)
  x = Dense(hidden_dim, activation='relu', name='fc6')(x)
  x = Dense(hidden_dim, activation='relu', name='fc7')(x)
  out = Dense(num_classes, activation='softmax', name='fc8')(x)
  custom_vgg_model = Model(vgg_model.input, out)

  return custom_vgg_model

num_classes = 256

new_model = addModel(vgg_model, num_classes)

print(new_model.summary())

plot_model(new_model, to_file='model.png')

from keras.optimizers import RMSprop
from keras.callbacks import ModelCheckpoint, EarlyStopping

checkpoint = ModelCheckpoint("vgg_face2_weights.h5",
                             monitor = "val_loss",
                             mode="min",
                             save_best_only = True,
                             verbose =1)

callbacks = [checkpoint]


new_model.compile(loss= 'categorical_crossentropy',
                  optimizer = 'adam',
                  metrics = ['accuracy'])
epochs = 20

history = new_model.fit(
    train_generator,
    epochs=epochs,
    steps_per_epoch= train_generator.samples // batch_size,
    callbacks = callbacks,
    validation_data = validation_generator,
    validation_steps = validation_generator.samples // batch_size,
)

new_model.save("vgg_face2_weights.h5")

#test the model
loss0, accuracy0 = new_model.evaluate(test_generator)

"""#Load model"""

# model trained with 256 classes from CASIA-WebFace
my_model = tf.keras.models.load_model('/content/drive/MyDrive/trained_models/vgg_face2_weights.h5')
my_model.summary()
